{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling - [Your Project Name Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Code Imports - Do not delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T18:04:18.914186Z",
     "start_time": "2019-07-17T18:04:18.900031Z"
    },
    "tags": [
     "hidecode"
    ]
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THESE\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T18:04:19.716372Z",
     "start_time": "2019-07-17T18:04:19.688612Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE This\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T18:04:21.946537Z",
     "start_time": "2019-07-17T18:04:20.793462Z"
    }
   },
   "outputs": [],
   "source": [
    "## DO NOT REMOVE\n",
    "## import local src module -\n",
    "## src in this project will contain all your local code\n",
    "## clean_data.py, model.py, visualize.py, custom.py\n",
    "from src import make_data as mk\n",
    "from src import visualize as viz\n",
    "from src import model as mdl\n",
    "from src import pandas_operators as po\n",
    "\n",
    "def test_src():\n",
    "    mk.test_make_data()\n",
    "    viz.test_viz()\n",
    "    mdl.test_model()\n",
    "    po.test_pandas()\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T18:04:22.049530Z",
     "start_time": "2019-07-17T18:04:22.025336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In make_data\n",
      "In Visualize\n",
      "In Model\n",
      "In pandas ops\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_src()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T18:04:25.071562Z",
     "start_time": "2019-07-17T18:04:25.049099Z"
    }
   },
   "outputs": [],
   "source": [
    "# For Dataframes and arrays\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Train:Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Modelling\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Activation, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(123)\n",
    "tf.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing data\n",
    "\n",
    "Most of the computational tools that we will be using cannot handle missing values, or at the very least produce unpredictable results.  We must therefore address these missing values in our dataset.  We have several options for this\n",
    "- eliminate missing values: This is a very simple method however we may end up removing too many samples, which would make it impossible for our model to distinguish between classes.  \n",
    "- impute missing values: This involves guessing what the values could be using values such as mean, median or even mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imr = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imr = imr.fit(df.values)\n",
    "imputed_data = imr.transform(df.values)\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out different models, as no model is always best\n",
    "For this project we are classifying.  There are many choices of classification algorithm, each with its own strengths and weaknesses.  There is no single classifier that always works best across all scenarios so we will compare a handful of different learning algorithms to select the best model for our particular problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided not to use the perceptron algorithm because our data set is not perfectly linearly separable, and so the algorithm will never converge.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use KNN\n",
    "KNN is a instance-based learning type of nonparamteric model.  It memorizes the training dataset and adapts immediately as we collect new training data.  \n",
    "\n",
    "The downside of KNN is that the computational complexity for classifying new samples grows linearly with the number of samples in the training dataset.  i.e. with every fight that occurs, and updates the model, the model becomes slower and slower to run.  Our dataset is relatively very small so we are able to use this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, \n",
    "                           p=2, \n",
    "                           metric='minkowski')\n",
    "knn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we represent what KNN is doing, using the age of fighters and the win percentage leading up to the fight.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the division of above\n",
    "X_age_winpct = data['age', 'win_pct']  # choose 2 x columns to show\n",
    "\n",
    "plot_decision_regions(X_age_winpct, y_combined, classifier=knn, test_idx=range(105, 150))\n",
    "# Change the test_idx to whatever subset you want to show\n",
    "\n",
    "plt.xlabel('Age of Fighter (years)')\n",
    "plt.ylabel('Win Percentage of fighter before the fight')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right number of neighbors (k) is critical to avoid over and underfitting our model.  \n",
    "We are using a Minkowski distance, which requires our distances to be standardized.  \n",
    "We do not have to regularize our data but we should use feature selection and dimensionality reduction techniques to avoid the \"curse of dimensionality\" (which would cause our model to overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we use a sigmoid function\n",
    "Our goal is to predict the probability that a certain sample belongs to a particular class, given its features.  In other words the probability that a certain matchup of two fighters will result in a win for fighter 1.  \n",
    "\n",
    "For this reason, we will use the logistic sigmoid function (abbreviated to sigmoid function) as our activation function.   The sigmoid function takes in any real number value and outputs a value between 0 and 1, which will represent the probability that we are after.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OvR Logistic Regression for multi-class classification\n",
    "Logistic regression models only really work for binary classification tasks, limiting us to only predicting win or loss for a fighter in a fight.  There is however one-vs-rest (also known as one-vs-all) logistic regression (OvR) which supports multi-class classification.  Scikit-learn enables us to use OvR logistic regression so we can essentially predict a win, loss or draw (3 classes) for each fighter within a fight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Logistic Regression Model\n",
    "lr = LogisticRegression(C=100.0, random_state=1)\n",
    "    # We set the C argument as 100 here.  a lower value will cause an increase in the regularization strength\n",
    "\n",
    "# Fit the Logistic Regression Model    \n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Show the predicted class for each observation\n",
    "lr.predict(X_test_std[:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization to help with overfitting and underfitting our model\n",
    "Overfitting is one issue we face when using machine learning models.  This is where our model performs very well with the training data that we provide, but is unable to perform well on the test data.  Overfitting can be caused by a number of factors including having too many parameters.  This would then lead to a model that is too complex for the underlying data.  \n",
    "There exists a trade off between overfitting and underfitting our model.  If a model is overfitting, the model is said to have high variance, and if the model is underfitting it is said to have high bias.  This bias-variance tradeoff can be dealt with by regularization.  Regularization will reduce the complexity of the model by accounting for high correlation between features (collinearity) and filtering out noise from our data. \n",
    "\n",
    "Regularization works by penalizing any extreme weights that we have.  One of the most common methods of regularization is L2 regularization (also known as Ridge Regression).  With the code example above, the \"C\" argument allows us to regularize the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation\n",
    "A benefit of using logitic regression is that the resulting logistic cost function is convex (U-shaped).  This makes it very easy to find the global cost minimum.  When we incorporate a logistic activation function into a multi-layer neural network however this U shape becomes more uneven, resulting in several local minima.  These local minima can \"trap\" our optimization algorithm, i.e. prevent our model from reaching the global minimum.  To help improve our model we can take advantage of backpropagation.  This will help us to reach a more satisfactory local minimum that yields powerful enough results (high accuracy in the case of this project).    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T15:48:06.434833Z",
     "start_time": "2019-07-19T15:48:06.315871Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show the contents of the processed data folder\n",
    "!ls ../data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T13:25:28.468964Z",
     "start_time": "2019-07-18T13:25:28.330069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: ../../data/: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!../../data/ ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data_clean = None\n",
    "data = pd.read_csv('../data/processed/{}'.format(data_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train:Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('outcome', axis=1)\n",
    "y = data['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "# we use a test_size of 0.3, i.e. 30% of the data has been held out, and we will use 70% of the data to train our model\n",
    "# we set the random_state so that our results are reproducable\n",
    "# we stratify so that we maintain the proportion of class labels, i.e. the same proportion of red wins and blue wins\n",
    "\n",
    "print('X_Train Rows: {}, Columns: {}'.format(X_train.shape[0], X_train.shape[1]))\n",
    "print('X_Test Rows: {}, Columns: {}'.format(X_test.shape[0], X_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the machine learning and optimization algorithms that we will be using require feature scaling in order to optimize performance.  We will standardize the features using StandardScaler from scikit-learn's preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same scaling parameters to standardize the test set, so that the values in the training and test dataset are comparable to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Centering and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "    # Centered\n",
    "X_train_centered = (X_train - mean_vals) / std_val\n",
    "X_test_centered  = (X_test  - mean_vals) / std_val\n",
    "    # No longer nead the X_train or X_test\n",
    "del X_train, X_test\n",
    "print('X_train shape: {} \\n y_train shape: {}'.format((X_train_centered.shape, y_train.shape))\n",
    "print('X_testshape: {} \\n y_test shape: {}'.format((X_test_centered.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding for Class Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = keras.utils.to_categorical(y_train)\n",
    "print('First 3 labels: ', y_train[:3])\n",
    "print('\\nFirst 3 labels (one-hot): \\n', y_train_onehot[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Neural Network with tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tanh = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T13:12:40.131773Z",
     "start_time": "2019-07-18T13:12:39.888672Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d45df69c23f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.add(\n\u001b[0m\u001b[1;32m      2\u001b[0m     keras.layers.Dense(\n\u001b[1;32m      3\u001b[0m         \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_centered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glorot_uniform'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Input Layer\n",
    "model_tanh.add(Dense(64, input_dim=X_train_centered.shape[1], kernel_initializer = 'glorot_uniform', \n",
    "                bias_initializer = 'zeros', activation = 'tanh'))\n",
    "\n",
    "# Hidden Layer 1\n",
    "model_tanh.add(Dense(64, input_dim = 64, kernel_initializer = 'glorot_uniform',\n",
    "                bias_initializer = 'zeros', activation = 'tanh'))\n",
    "model_tanh.add(Drouput(0.5, seed=123))\n",
    "\n",
    "# Hidden Layer 2\n",
    "model_tanh.add(Dense(units = y_train_onehot.shape[1], input_dim = 64, kernel_initializer = 'glorot_uniform', \n",
    "                bias_initializer = 'zeros', activation = 'softmax'))\n",
    "model_tanh.add(Drouput(0.5, seed=123))\n",
    "\n",
    "# Output Layer\n",
    "sgd_optimizer = SGD(lr=0.001, decay=1e-7, momentum=0.9) #learn rate, weight decay constant, momentum learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a tradeoff when we are finding an appropriate learning rate.  \n",
    "- Too large and the alorithm may overshoot the global cost minimum.  \n",
    "- Too small and the algorithm requires far more epochs until convergence, which results in unecessary computational energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tanh.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model_tanh.fit(x_train, y_train, epochs=20, batch_size=128)\n",
    "score = model_tanh.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Neural Network with leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lrelu = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T13:12:40.131773Z",
     "start_time": "2019-07-18T13:12:39.888672Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d45df69c23f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.add(\n\u001b[0m\u001b[1;32m      2\u001b[0m     keras.layers.Dense(\n\u001b[1;32m      3\u001b[0m         \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_centered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glorot_uniform'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Input Layer\n",
    "model_lrelu.add(Dense(64, input_dim=X_train_centered.shape[1], kernel_initializer = 'glorot_uniform', \n",
    "                bias_initializer = 'zeros'))\n",
    "model_lrelu.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "# Hidden Layer 1\n",
    "model_lrelu.add(Dense(64, input_dim = 64, kernel_initializer = 'glorot_uniform', bias_initializer = 'zeros'))\n",
    "model_lrelu.add(LeakyReLU(alpha=0.01))\n",
    "model_lrelu.add(Drouput(0.5, seed=123))\n",
    "\n",
    "# Hidden Layer 2\n",
    "model_lrelu.add(Dense(units = y_train_onehot.shape[1], input_dim = 64, kernel_initializer = 'glorot_uniform', \n",
    "                bias_initializer = 'zeros', activation = 'softmax'))\n",
    "model_lrelu.add(LeakyReLU(alpha=0.01))\n",
    "model_lrelu.add(Drouput(0.5, seed=123))\n",
    "\n",
    "# Output Layer\n",
    "sgd_optimizer = SGD(lr=0.001, decay=1e-7, momentum=0.9) #learn rate, weight decay constant, momentum learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lrelu.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model_lrelu.fit(x_train, y_train, epochs=20, batch_size=128)\n",
    "score = model_lrelu.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_centered, y_train_onehot,\n",
    "                    batch_size=64, epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)  # verbose lets us follow optimization of cost function, validation split helps monitor if model is overfitting in training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose=0)\n",
    "print('First 3 predictions: ', y_train_pred[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose=0)\n",
    "print('First 3 predictions: ', y_train_pred[:3])\n",
    "\n",
    "# Training Accuracy\n",
    "correct_preds = np.sum(y_train == y_train_pred, axis=0)\n",
    "train_acc = correct_preds / y_train.shape[0]\n",
    "print('Training Accuracy: {}%'.format(train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Accuracy\n",
    "y_test_pred = model.predict_classes(X_test_centered, verbose=0)\n",
    "correct_preds = np.sum(y_test == y_test_pred, axis=0)\n",
    "test_acc = correct_preds / y_test.shape[0]\n",
    "print('Test Accuracy: {}%'.format(test_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
