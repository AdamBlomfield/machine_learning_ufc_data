{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling - [Your Project Name Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Code Imports - Do not delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T21:11:05.792010Z",
     "start_time": "2019-07-21T21:11:05.777404Z"
    },
    "tags": [
     "hidecode"
    ]
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THESE\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T21:11:05.996795Z",
     "start_time": "2019-07-21T21:11:05.974892Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE This\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T21:11:06.858857Z",
     "start_time": "2019-07-21T21:11:06.163120Z"
    }
   },
   "outputs": [],
   "source": [
    "## DO NOT REMOVE\n",
    "## import local src module -\n",
    "## src in this project will contain all your local code\n",
    "## clean_data.py, model.py, visualize.py, custom.py\n",
    "from src import make_data as mk\n",
    "from src import visualize as viz\n",
    "from src import model as mdl\n",
    "from src import pandas_operators as po\n",
    "\n",
    "def test_src():\n",
    "    mk.test_make_data()\n",
    "    viz.test_viz()\n",
    "    mdl.test_model()\n",
    "    po.test_pandas()\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T21:11:06.926760Z",
     "start_time": "2019-07-21T21:11:06.906259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In make_data\n",
      "In Visualize\n",
      "In Model\n",
      "In pandas ops\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_src()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:36:54.173252Z",
     "start_time": "2019-07-21T22:36:54.138405Z"
    }
   },
   "outputs": [],
   "source": [
    "# For Dataframes and arrays\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Train:Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Modelling\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Activation, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(123)\n",
    "tf.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out different models, as no model is always best\n",
    "For this project we are classifying.  There are many choices of classification algorithm, each with its own strengths and weaknesses.  There is no single classifier that always works best across all scenarios so we will compare a handful of different learning algorithms to select the best model for our particular problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided not to use the perceptron algorithm because our data set is not perfectly linearly separable, and so the algorithm will never converge.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use KNN\n",
    "KNN is a instance-based learning type of nonparamteric model.  It memorizes the training dataset and adapts immediately as we collect new training data.  \n",
    "\n",
    "The downside of KNN is that the computational complexity for classifying new samples grows linearly with the number of samples in the training dataset.  i.e. with every fight that occurs, and updates the model, the model becomes slower and slower to run.  Our dataset is relatively very small so we are able to use this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, \n",
    "                           p=2, \n",
    "                           metric='minkowski')\n",
    "knn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we represent what KNN is doing, using the age of fighters and the win percentage leading up to the fight.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the division of above\n",
    "X_age_winpct = data['age', 'win_pct']  # choose 2 x columns to show\n",
    "\n",
    "plot_decision_regions(X_age_winpct, y_combined, classifier=knn, test_idx=range(105, 150))\n",
    "# Change the test_idx to whatever subset you want to show\n",
    "\n",
    "plt.xlabel('Age of Fighter (years)')\n",
    "plt.ylabel('Win Percentage of fighter before the fight')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right number of neighbors (k) is critical to avoid over and underfitting our model.  \n",
    "We are using a Minkowski distance, which requires our distances to be standardized.  \n",
    "We do not have to regularize our data but we should use feature selection and dimensionality reduction techniques to avoid the \"curse of dimensionality\" (which would cause our model to overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we use a sigmoid function\n",
    "Our goal is to predict the probability that a certain sample belongs to a particular class, given its features.  In other words the probability that a certain matchup of two fighters will result in a win for fighter 1.  \n",
    "\n",
    "For this reason, we will use the logistic sigmoid function (abbreviated to sigmoid function) as our activation function.   The sigmoid function takes in any real number value and outputs a value between 0 and 1, which will represent the probability that we are after.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OvR Logistic Regression for multi-class classification\n",
    "Logistic regression models only really work for binary classification tasks, limiting us to only predicting win or loss for a fighter in a fight.  There is however one-vs-rest (also known as one-vs-all) logistic regression (OvR) which supports multi-class classification.  Scikit-learn enables us to use OvR logistic regression so we can essentially predict a win, loss or draw (3 classes) for each fighter within a fight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Logistic Regression Model\n",
    "lr = LogisticRegression(C=100.0, random_state=1)\n",
    "    # We set the C argument as 100 here.  a lower value will cause an increase in the regularization strength\n",
    "\n",
    "# Fit the Logistic Regression Model    \n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Show the predicted class for each observation\n",
    "lr.predict(X_test_std[:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization to help with overfitting and underfitting our model\n",
    "Overfitting is one issue we face when using machine learning models.  This is where our model performs very well with the training data that we provide, but is unable to perform well on the test data.  Overfitting can be caused by a number of factors including having too many parameters.  This would then lead to a model that is too complex for the underlying data.  \n",
    "There exists a trade off between overfitting and underfitting our model.  If a model is overfitting, the model is said to have high variance, and if the model is underfitting it is said to have high bias.  This bias-variance tradeoff can be dealt with by regularization.  Regularization will reduce the complexity of the model by accounting for high correlation between features (collinearity) and filtering out noise from our data. \n",
    "\n",
    "Regularization works by penalizing any extreme weights that we have.  One of the most common methods of regularization is L2 regularization (also known as Ridge Regression).  With the code example above, the \"C\" argument allows us to regularize the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation\n",
    "A benefit of using logitic regression is that the resulting logistic cost function is convex (U-shaped).  This makes it very easy to find the global cost minimum.  When we incorporate a logistic activation function into a multi-layer neural network however this U shape becomes more uneven, resulting in several local minima.  These local minima can \"trap\" our optimization algorithm, i.e. prevent our model from reaching the global minimum.  To help improve our model we can take advantage of backpropagation.  This will help us to reach a more satisfactory local minimum that yields powerful enough results (high accuracy in the case of this project).    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T21:50:34.995476Z",
     "start_time": "2019-07-21T21:50:34.839170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bouts_cleaned    combined         fighters_cleaned\r\n"
     ]
    }
   ],
   "source": [
    "# Show the contents of the processed data folder\n",
    "!ls ../data/processed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:30:46.576469Z",
     "start_time": "2019-07-21T22:30:46.514389Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/processed/combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:30:47.042434Z",
     "start_time": "2019-07-21T22:30:47.001311Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "      <th>fighter1</th>\n",
       "      <th>fighter2</th>\n",
       "      <th>winner_is_fighter1</th>\n",
       "      <th>title_fight</th>\n",
       "      <th>method_DEC</th>\n",
       "      <th>method_DQ</th>\n",
       "      <th>method_KO/TKO</th>\n",
       "      <th>method_SUB</th>\n",
       "      <th>...</th>\n",
       "      <th>fighter2_dob</th>\n",
       "      <th>fighter2_age_today</th>\n",
       "      <th>fighter2_slpm</th>\n",
       "      <th>fighter2_str_acc</th>\n",
       "      <th>fighter2_sapm</th>\n",
       "      <th>fighter2_str_def</th>\n",
       "      <th>fighter2_td_avg</th>\n",
       "      <th>fighter2_td_acc</th>\n",
       "      <th>fighter2_td_def</th>\n",
       "      <th>fighter2_sub_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-17</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Santiago Ponzinibbio</td>\n",
       "      <td>Neil Magny</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1987-08-03</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.86</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.22</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.62</td>\n",
       "      <td>46.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-17</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ricardo Lamas</td>\n",
       "      <td>Darren Elkins</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1984-05-16</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.36</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.83</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.68</td>\n",
       "      <td>35.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-17</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Khalil Rountree Jr.</td>\n",
       "      <td>Johnny Walker</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1992-03-30</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.37</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3.36</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   location              fighter1       fighter2  \\\n",
       "0  2018-11-17  Argentina  Santiago Ponzinibbio     Neil Magny   \n",
       "1  2018-11-17  Argentina         Ricardo Lamas  Darren Elkins   \n",
       "2  2018-11-17  Argentina   Khalil Rountree Jr.  Johnny Walker   \n",
       "\n",
       "   winner_is_fighter1  title_fight  method_DEC  method_DQ  method_KO/TKO  \\\n",
       "0                 1.0          0.0         0.0        0.0            1.0   \n",
       "1                 1.0          0.0         0.0        0.0            1.0   \n",
       "2                 0.0          0.0         0.0        0.0            1.0   \n",
       "\n",
       "   method_SUB        ...         fighter2_dob  fighter2_age_today  \\\n",
       "0         0.0        ...           1987-08-03                32.0   \n",
       "1         0.0        ...           1984-05-16                35.0   \n",
       "2         0.0        ...           1992-03-30                27.0   \n",
       "\n",
       "   fighter2_slpm  fighter2_str_acc  fighter2_sapm  fighter2_str_def  \\\n",
       "0           3.86              46.0           2.22              56.0   \n",
       "1           3.36              37.0           2.83              53.0   \n",
       "2           5.37              70.0           3.36              25.0   \n",
       "\n",
       "   fighter2_td_avg fighter2_td_acc fighter2_td_def  fighter2_sub_avg  \n",
       "0             2.62            46.0            60.0               0.3  \n",
       "1             2.68            35.0            57.0               1.3  \n",
       "2             0.89           100.0           100.0               2.6  \n",
       "\n",
       "[3 rows x 46 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use shortened data until 227 values fixed and 226 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:30:48.535673Z",
     "start_time": "2019-07-21T22:30:48.506036Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data[:-226]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:30:49.428116Z",
     "start_time": "2019-07-21T22:30:49.393442Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                  226\n",
       "location              226\n",
       "fighter1              226\n",
       "fighter2              226\n",
       "winner_is_fighter1    226\n",
       "title_fight           226\n",
       "method_DEC            226\n",
       "method_DQ             226\n",
       "method_KO/TKO         226\n",
       "method_SUB            226\n",
       "dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:30:50.120494Z",
     "start_time": "2019-07-21T22:30:50.089972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:30:51.809919Z",
     "start_time": "2019-07-21T22:30:51.778042Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data[~data.winner_is_fighter1.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train:Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:30:54.504035Z",
     "start_time": "2019-07-21T22:30:54.474657Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'location',\n",
       " 'fighter1',\n",
       " 'fighter2',\n",
       " 'winner_is_fighter1',\n",
       " 'title_fight',\n",
       " 'method_DEC',\n",
       " 'method_DQ',\n",
       " 'method_KO/TKO',\n",
       " 'method_SUB',\n",
       " 'fighter1_win',\n",
       " 'fighter1_lose',\n",
       " 'fighter1_draw',\n",
       " 'fighter1_total_bouts',\n",
       " 'fighter1_win_rate',\n",
       " 'fighter1_height_inches',\n",
       " 'fighter1_reach',\n",
       " 'fighter1_stance',\n",
       " 'fighter1_dob',\n",
       " 'fighter1_age_today',\n",
       " 'fighter1_slpm',\n",
       " 'fighter1_str_acc',\n",
       " 'fighter1_sapm',\n",
       " 'fighter1_str_def',\n",
       " 'fighter1_td_avg',\n",
       " 'fighter1_td_acc',\n",
       " 'fighter1_td_def',\n",
       " 'fighter1_sub_avg',\n",
       " 'fighter2_win',\n",
       " 'fighter2_lose',\n",
       " 'fighter2_draw',\n",
       " 'fighter2_total_bouts',\n",
       " 'fighter2_win_rate',\n",
       " 'fighter2_height_inches',\n",
       " 'fighter2_reach',\n",
       " 'fighter2_stance',\n",
       " 'fighter2_dob',\n",
       " 'fighter2_age_today',\n",
       " 'fighter2_slpm',\n",
       " 'fighter2_str_acc',\n",
       " 'fighter2_sapm',\n",
       " 'fighter2_str_def',\n",
       " 'fighter2_td_avg',\n",
       " 'fighter2_td_acc',\n",
       " 'fighter2_td_def',\n",
       " 'fighter2_sub_avg']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = data.columns.tolist()\n",
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:30:58.978467Z",
     "start_time": "2019-07-21T22:30:58.936291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "      <th>fighter1</th>\n",
       "      <th>fighter2</th>\n",
       "      <th>winner_is_fighter1</th>\n",
       "      <th>title_fight</th>\n",
       "      <th>method_DEC</th>\n",
       "      <th>method_DQ</th>\n",
       "      <th>method_KO/TKO</th>\n",
       "      <th>method_SUB</th>\n",
       "      <th>...</th>\n",
       "      <th>fighter2_dob</th>\n",
       "      <th>fighter2_age_today</th>\n",
       "      <th>fighter2_slpm</th>\n",
       "      <th>fighter2_str_acc</th>\n",
       "      <th>fighter2_sapm</th>\n",
       "      <th>fighter2_str_def</th>\n",
       "      <th>fighter2_td_avg</th>\n",
       "      <th>fighter2_td_acc</th>\n",
       "      <th>fighter2_td_def</th>\n",
       "      <th>fighter2_sub_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-17</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Santiago Ponzinibbio</td>\n",
       "      <td>Neil Magny</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1987-08-03</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.86</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.22</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.62</td>\n",
       "      <td>46.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-17</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ricardo Lamas</td>\n",
       "      <td>Darren Elkins</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1984-05-16</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.36</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.83</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.68</td>\n",
       "      <td>35.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-17</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Khalil Rountree Jr.</td>\n",
       "      <td>Johnny Walker</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1992-03-30</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.37</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3.36</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   location              fighter1       fighter2  \\\n",
       "0  2018-11-17  Argentina  Santiago Ponzinibbio     Neil Magny   \n",
       "1  2018-11-17  Argentina         Ricardo Lamas  Darren Elkins   \n",
       "2  2018-11-17  Argentina   Khalil Rountree Jr.  Johnny Walker   \n",
       "\n",
       "   winner_is_fighter1  title_fight  method_DEC  method_DQ  method_KO/TKO  \\\n",
       "0                 1.0          0.0         0.0        0.0            1.0   \n",
       "1                 1.0          0.0         0.0        0.0            1.0   \n",
       "2                 0.0          0.0         0.0        0.0            1.0   \n",
       "\n",
       "   method_SUB        ...         fighter2_dob  fighter2_age_today  \\\n",
       "0         0.0        ...           1987-08-03                32.0   \n",
       "1         0.0        ...           1984-05-16                35.0   \n",
       "2         0.0        ...           1992-03-30                27.0   \n",
       "\n",
       "   fighter2_slpm  fighter2_str_acc  fighter2_sapm  fighter2_str_def  \\\n",
       "0           3.86              46.0           2.22              56.0   \n",
       "1           3.36              37.0           2.83              53.0   \n",
       "2           5.37              70.0           3.36              25.0   \n",
       "\n",
       "   fighter2_td_avg fighter2_td_acc fighter2_td_def  fighter2_sub_avg  \n",
       "0             2.62            46.0            60.0               0.3  \n",
       "1             2.68            35.0            57.0               1.3  \n",
       "2             0.89           100.0           100.0               2.6  \n",
       "\n",
       "[3 rows x 46 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:31:00.513172Z",
     "start_time": "2019-07-21T22:31:00.479551Z"
    }
   },
   "outputs": [],
   "source": [
    "categories = ['title_fight', 'method_DEC', 'method_DQ', 'method_KO/TKO', 'method_SUB',\n",
    " 'fighter1_win', 'fighter1_lose', 'fighter1_draw', 'fighter1_total_bouts', 'fighter1_win_rate', 'fighter1_height_inches',\n",
    " 'fighter1_reach', 'fighter1_age_today', \n",
    " 'fighter1_slpm', 'fighter1_str_acc', 'fighter1_sapm', 'fighter1_str_def',\n",
    " 'fighter1_td_avg', 'fighter1_td_acc', 'fighter1_td_def','fighter1_sub_avg',\n",
    " 'fighter2_win', 'fighter2_lose', 'fighter2_draw', 'fighter2_total_bouts', 'fighter2_win_rate', 'fighter2_height_inches',\n",
    " 'fighter2_reach', 'fighter2_age_today',\n",
    " 'fighter2_slpm', 'fighter2_str_acc', 'fighter2_sapm', 'fighter2_str_def',\n",
    " 'fighter2_td_avg', 'fighter2_td_acc', 'fighter2_td_def', 'fighter2_sub_avg']\n",
    "\n",
    "X = data[categories]\n",
    "y = data['winner_is_fighter1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:31:01.091095Z",
     "start_time": "2019-07-21T22:31:01.058828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train Rows: 3040, Columns: 37\n",
      "X_Test Rows: 1304, Columns: 37\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "# we use a test_size of 0.3, i.e. 30% of the data has been held out, and we will use 70% of the data to train our model\n",
    "# we set the random_state so that our results are reproducable\n",
    "# we stratify so that we maintain the proportion of class labels, i.e. the same proportion of red wins and blue wins\n",
    "\n",
    "print('X_Train Rows: {}, Columns: {}'.format(X_train.shape[0], X_train.shape[1]))\n",
    "print('X_Test Rows: {}, Columns: {}'.format(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the machine learning and optimization algorithms that we will be using require feature scaling in order to optimize performance.  We will standardize the features using StandardScaler from scikit-learn's preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:31:07.903292Z",
     "start_time": "2019-07-21T22:31:07.870421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3040 entries, 2477 to 2270\n",
      "Data columns (total 37 columns):\n",
      "title_fight               3040 non-null float64\n",
      "method_DEC                3040 non-null float64\n",
      "method_DQ                 3040 non-null float64\n",
      "method_KO/TKO             3040 non-null float64\n",
      "method_SUB                3040 non-null float64\n",
      "fighter1_win              3040 non-null float64\n",
      "fighter1_lose             3040 non-null float64\n",
      "fighter1_draw             3040 non-null float64\n",
      "fighter1_total_bouts      3040 non-null float64\n",
      "fighter1_win_rate         3040 non-null float64\n",
      "fighter1_height_inches    3040 non-null float64\n",
      "fighter1_reach            3040 non-null float64\n",
      "fighter1_age_today        3023 non-null float64\n",
      "fighter1_slpm             3040 non-null float64\n",
      "fighter1_str_acc          3040 non-null float64\n",
      "fighter1_sapm             3040 non-null float64\n",
      "fighter1_str_def          3040 non-null float64\n",
      "fighter1_td_avg           3040 non-null float64\n",
      "fighter1_td_acc           3040 non-null float64\n",
      "fighter1_td_def           3040 non-null float64\n",
      "fighter1_sub_avg          3040 non-null float64\n",
      "fighter2_win              3040 non-null float64\n",
      "fighter2_lose             3040 non-null float64\n",
      "fighter2_draw             3040 non-null float64\n",
      "fighter2_total_bouts      3040 non-null float64\n",
      "fighter2_win_rate         3040 non-null float64\n",
      "fighter2_height_inches    3040 non-null float64\n",
      "fighter2_reach            3040 non-null float64\n",
      "fighter2_age_today        3029 non-null float64\n",
      "fighter2_slpm             3040 non-null float64\n",
      "fighter2_str_acc          3040 non-null float64\n",
      "fighter2_sapm             3040 non-null float64\n",
      "fighter2_str_def          3040 non-null float64\n",
      "fighter2_td_avg           3040 non-null float64\n",
      "fighter2_td_acc           3040 non-null float64\n",
      "fighter2_td_def           3040 non-null float64\n",
      "fighter2_sub_avg          3040 non-null float64\n",
      "dtypes: float64(37)\n",
      "memory usage: 902.5 KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:31:15.102836Z",
     "start_time": "2019-07-21T22:31:15.063609Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same scaling parameters to standardize the test set, so that the values in the training and test dataset are comparable to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:31:41.368344Z",
     "start_time": "2019-07-21T22:31:41.336474Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Centering and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "    # Centered\n",
    "X_train_centered = (X_train - mean_vals) / std_val\n",
    "X_test_centered  = (X_test  - mean_vals) / std_val\n",
    "    # No longer nead the X_train or X_test\n",
    "del X_train, X_test\n",
    "print('X_train shape: {} \\n y_train shape: {}'.format((X_train_centered.shape, y_train.shape))\n",
    "print('X_testshape: {} \\n y_test shape: {}'.format((X_test_centered.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding for Class Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = keras.utils.to_categorical(y_train)\n",
    "print('First 3 labels: ', y_train[:3])\n",
    "print('\\nFirst 3 labels (one-hot): \\n', y_train_onehot[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Neural Network with tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:34:02.153686Z",
     "start_time": "2019-07-21T22:34:02.123660Z"
    }
   },
   "outputs": [],
   "source": [
    "model_tanh = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:39:00.018540Z",
     "start_time": "2019-07-21T22:38:59.946778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input Layer\n",
    "model_tanh.add(Dense(64, input_dim=X_train_std.shape[1], kernel_initializer = 'glorot_uniform', \n",
    "                bias_initializer = 'zeros', activation = 'tanh'))\n",
    "\n",
    "# Hidden Layer 1\n",
    "model_tanh.add(Dense(64, input_dim = 64, kernel_initializer = 'glorot_uniform',\n",
    "                bias_initializer = 'zeros', activation = 'tanh'))\n",
    "# model_tanh.add(Drouput(0.5, seed=123))\n",
    "\n",
    "# Hidden Layer 2\n",
    "model_tanh.add(Dense(units = X_train.shape[1], input_dim = 64, kernel_initializer = 'glorot_uniform', \n",
    "                bias_initializer = 'zeros', activation = 'softmax'))\n",
    "# model_tanh.add(Drouput(0.5, seed=123))\n",
    "\n",
    "# Output Layer\n",
    "sgd_optimizer = SGD(lr=0.001, decay=1e-7, momentum=0.9) #learn rate, weight decay constant, momentum learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a tradeoff when we are finding an appropriate learning rate.  \n",
    "- Too large and the alorithm may overshoot the global cost minimum.  \n",
    "- Too small and the algorithm requires far more epochs until convergence, which results in unecessary computational energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:39:46.932954Z",
     "start_time": "2019-07-21T22:39:46.862690Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_15 to have shape (37,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-8edb5a88441a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_tanh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_tanh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_tanh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml-ufc-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml-ufc-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml-ufc-env/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_15 to have shape (37,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "model_tanh.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model_tanh.fit(X_train, y_train, epochs=20, batch_size=128)\n",
    "score = model_tanh.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Neural Network with leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:39:49.618586Z",
     "start_time": "2019-07-21T22:39:49.585983Z"
    }
   },
   "outputs": [],
   "source": [
    "model_lrelu = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:40:20.522042Z",
     "start_time": "2019-07-21T22:40:20.459043Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input Layer\n",
    "model_lrelu.add(Dense(64, input_dim=X_train_std.shape[1], kernel_initializer = 'glorot_uniform', \n",
    "                bias_initializer = 'zeros'))\n",
    "model_lrelu.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "# Hidden Layer 1\n",
    "model_lrelu.add(Dense(64, input_dim = 64, kernel_initializer = 'glorot_uniform', bias_initializer = 'zeros'))\n",
    "model_lrelu.add(LeakyReLU(alpha=0.01))\n",
    "# model_lrelu.add(Drouput(0.5, seed=123))\n",
    "\n",
    "# Hidden Layer 2\n",
    "# model_lrelu.add(Dense(units = y_train.shape[1], input_dim = 64, kernel_initializer = 'glorot_uniform', \n",
    "#                 bias_initializer = 'zeros', activation = 'softmax'))\n",
    "# model_lrelu.add(LeakyReLU(alpha=0.01))\n",
    "# model_lrelu.add(Drouput(0.5, seed=123))\n",
    "\n",
    "# Output Layer\n",
    "sgd_optimizer = SGD(lr=0.001, decay=1e-7, momentum=0.9) #learn rate, weight decay constant, momentum learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:40:31.139090Z",
     "start_time": "2019-07-21T22:40:31.071883Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected leaky_re_lu_8 to have shape (64,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-2a80fb85d39c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_lrelu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_lrelu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_lrelu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml-ufc-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml-ufc-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml-ufc-env/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected leaky_re_lu_8 to have shape (64,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "model_lrelu.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model_lrelu.fit(X_train, y_train, epochs=20, batch_size=128)\n",
    "score = model_lrelu.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:41:33.985847Z",
     "start_time": "2019-07-21T22:41:33.938533Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_15 to have shape (37,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-eaa080185566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_split=0.1)  # verbose lets us follow optimization of cost function, validation split helps monitor if model is overfitting in training\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/ml-ufc-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml-ufc-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml-ufc-env/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_15 to have shape (37,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "history = model_tanh.fit(X_train_std, y_train,\n",
    "                    batch_size=64, epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)  # verbose lets us follow optimization of cost function, validation split helps monitor if model is overfitting in training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:41:37.900157Z",
     "start_time": "2019-07-21T22:41:37.852607Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected leaky_re_lu_8 to have shape (64,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-c81f428fc54a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_split=0.1)  # verbose lets us follow optimization of cost function, validation split helps monitor if model is overfitting in training\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/ml-ufc-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml-ufc-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml-ufc-env/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected leaky_re_lu_8 to have shape (64,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "history = model_lrelu.fit(X_train_std, y_train,\n",
    "                    batch_size=64, epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)  # verbose lets us follow optimization of cost function, validation split helps monitor if model is overfitting in training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:42:08.206041Z",
     "start_time": "2019-07-21T22:42:07.919414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 predictions:  [12 24 23]\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model_tanh.predict_classes(X_train_std, verbose=0)\n",
    "print('First 3 predictions: ', y_train_pred[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:42:10.488957Z",
     "start_time": "2019-07-21T22:42:10.394877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 predictions:  [34 34 34]\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model_lrelu.predict_classes(X_train_std, verbose=0)\n",
    "print('First 3 predictions: ', y_train_pred[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:42:52.763923Z",
     "start_time": "2019-07-21T22:42:52.680615Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 predictions:  [12 24 23]\n",
      "Training Accuracy: 2.3355263157894735%\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model_tanh.predict_classes(X_train_std, verbose=0)\n",
    "print('First 3 predictions: ', y_train_pred[:3])\n",
    "\n",
    "# Training Accuracy\n",
    "correct_preds = np.sum(y_train == y_train_pred, axis=0)\n",
    "train_acc = correct_preds / y_train.shape[0]\n",
    "print('Training Accuracy: {}%'.format(train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:43:09.929132Z",
     "start_time": "2019-07-21T22:43:09.868634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 2.147239263803681%\n"
     ]
    }
   ],
   "source": [
    "# Testing Accuracy\n",
    "y_test_pred = model_tanh.predict_classes(X_test_std, verbose=0)\n",
    "correct_preds = np.sum(y_test == y_test_pred, axis=0)\n",
    "test_acc = correct_preds / y_test.shape[0]\n",
    "print('Test Accuracy: {}%'.format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky RELU Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:43:26.179255Z",
     "start_time": "2019-07-21T22:43:26.101365Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 predictions:  [34 34 34]\n",
      "Training Accuracy: 0.5263157894736842%\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model_lrelu.predict_classes(X_train_std, verbose=0)\n",
    "print('First 3 predictions: ', y_train_pred[:3])\n",
    "\n",
    "# Training Accuracy\n",
    "correct_preds = np.sum(y_train == y_train_pred, axis=0)\n",
    "train_acc = correct_preds / y_train.shape[0]\n",
    "print('Training Accuracy: {}%'.format(train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T22:43:24.743855Z",
     "start_time": "2019-07-21T22:43:24.693582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5368098159509203%\n"
     ]
    }
   ],
   "source": [
    "# Testing Accuracy\n",
    "y_test_pred = model_lrelu.predict_classes(X_test_std, verbose=0)\n",
    "correct_preds = np.sum(y_test == y_test_pred, axis=0)\n",
    "test_acc = correct_preds / y_test.shape[0]\n",
    "print('Test Accuracy: {}%'.format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
